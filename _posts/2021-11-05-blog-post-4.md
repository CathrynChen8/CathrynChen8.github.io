---
layout: post
title: Blog Post 4
---

# Blog Post 4: Spectral Clustering

## Introduction

In this problem, we'll do *spectral clustering* to classify the data clustering, especially with complex structure where we cannot do it with kmeans model.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```
Here are examples we use other methdos to clustering the point.

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
![png](/images/output_2_1.png)


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![png](/images/output_2.png)


### Harder Clustering

What if our data is more complex, like contain rotation and other shape.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

![png](/images/output_6_1.png)


We can still try kmeans model, while this is not so good this times, because kmeans is mean to find circular cluster, while in this data we have curve that kinds of mixed to each other in circle.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_8_1.png)


This time we can see that the classification is not correct, we have part of purple on the curve above, and part of yellow on the curve below.


## Part A

We use epsilon = 0.4, and comparing it with thw eulidean distance between X. If the distance between Xi and Xj is smaller than the epsilon, the entry of A (aij) will be 1, else the entry will be 0. We will set the diagonal of A to be 0 since it is related to itself. A offers a view of the similarity and the distance relationship between X.


```python
from sklearn.metrics import pairwise_distances
#pairwise_distance function in sklearn.metrix help calculate the eucliean distance by default between two points and produces matrix from that
A  = pairwise_distances(X) 

epsilon = 0.4

#set the diagonal to 1 to make sure that it is greater than epsilon and thus we will get a 0 after test
np.fill_diagonal(A, 1) 

#test whether the distance is smaller than epsilon
A = A < epsilon **2

#change the true and false to number, 1 if smaller and 0 otherwise
A = A.astype(int)

A
```

    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           ...,
           [0, 0, 0, ..., 0, 0, 1],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 1, 0, 0]])


Here is the A we get.


## Part B


The *binary norm cut objective* of a matrix **A** is the function 

NA(c0, c1) = cut(C0, C1)((1/vol(C0)) + (1/vol(C1))

In this expression, 
- cut(c0, c1) here is equal to the sum of entries aij while i in in cluster C0, and j in cluster C1
- vol(c0) id equal to the sum of the sum of all column (degree of row i) of all i in c0. vol(c1) follows the same logic and return the sum of all i in c1.

From that, we will build of computation model.


#### B.1 The Cut Term

As mentioned above, we will develop function to calculate the cut part of the formula, which is the sum of all entries related C0 and C1.


```python
def cut(A, y):
    i0 = np.where(y==0)
    i1 = np.where(y==1)
    sum = 0
    for i in i0[0]:
        for j in i1[0]:
            sum = sum + A[i,j]
    return sum
```

When we compute it on our true model, we get a really small value:


```python
cut(A, y)
```


    0


When we try it on a randomly generated model with length A, we get a much higher value:

```python
randnums = np.random.randint(0,2,len(A)) #generate random vector of length 200 with 0, 1
cut(A, randnums)
```

    376



#### B.2 The Volume Term 

Now we look at the calculation of our vol equation to calculate the volumn of c0 and c1, by individually summing the entries and rows in C0 and C1 and return them at last,

```python
def vols(A,y):
    i0 = np.where(y==0) #find where y == 0, denote the index as i0
    i1 = np.where(y==1) #find where y == 1, denote the index as i1
    vol0 = sum(A[i0[0],:]) #sum the column of the row inside the i0
    vol1 = sum(A[i1[0],:]) #sum the column of the row inside the i1
    vol0 = sum(vol0) #sum all the number of i0
    vol1 = sum(vol1) #sum every number of i1
    return vol0, vol1
        
    
```

We will copmute the vols of our data:

```python
results = vols(A,y) #get the result based on A and y
results
```




    (774, 748)



We then by the norm cut formula, computing the final NA we want of the data.

```python
def normcut(A,y):
    result = cut(A,y) * ((1/vols(A,y)[0]) + (1/vols(A,y)[1])) #use the formula to calculate the result
    return result
```

Here is what we get for our data:

```python
normcut(A, y) #calculate the norm cut for A, y
```




    0.0


Here is the norm cut we get from our random data:


```python
normcut(A, randnums) #calculate the result based on random label
```




    0.9941491209783893



We can see that the normcut objective using the true labels y are much smaller than that of the fake labels I generated above, while one is close to 0, the other is close to 1.

## Part C

We define a vector z that takes 1/vol(c0) if y = 0, and takes -1/vol(c1) if y = 1.


We first builf up a function of this vector, named transform:

```python
import copy
def transform(A,y):
    z = np.zeros(len(A)) #initialize the vector
    z[y == 1] = -1/vols(A,y)[1] #get the number for y==1 by the formula
    z[y == 0] = 1/vols(A,y)[0] #get the number for y==0 by the formula
    return z
```


```python
z = transform(A,y) #apply the function to A and y
```


We then compute the D which is the diagonal matrix with nonzero diagonal, equal to the sum of a at that row, with every other entries = 0.

```python
D = np.zeros((len(A), len(A))) #initialize a matrix with zeros entries
np.fill_diagonal(D, sum(A[:,])) #fill the diagonal with the sum of column of that row value
```

We calculate the na by (z_transpose * (D-A) * z)/ (z_transpose * D * z), here is the number we get:

```python
right =  2 *(z @ (D - A) @z)/(z@D@z) #compute the right part of the equation we want to check
right
```




    -6.449592466169486e-17



We then compare it with the NA we gets from before steps:
```python
np.isclose(right, normcut(A, y)) #compare using np.isclose
```




    True


We can see that the two are equal.

When we compare the z * D * 1 with 0 :

```python
np.isclose(z@ D @np.ones(len(A)), 0) #compare the value with 0
```


    True


We can see that the two are equal as well.

## Part D

From the formula in part c, we find a different way to compute NA,  we will then use the orthogonal complement of z related to D1 to build up a function of the RA(NA).


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Then we use optimization function to minize the function, and return a minimizing vector.


```python
import scipy.optimize

z_ = scipy.optimize.minimize(orth_obj, np.ones(len(z))) #use the minimize function to minimize the orth_obj function and output a minimizing vector
z_ = z_.x #get the minimizing vector
```


## Part E

We then graph the dots with different colors based on the sign of the minimizing vector:


```python
col = np.where(z_ >= 0, 'r', np.where(z_ < 0, 'y' , 'b') #assign the color vector based on sign
plt.scatter(X[:,0], X[:,1], c = col) #draw the graph
```



![png](/images/output_39_1.png)


It looks like we correctly categorize the data.

## Part F

We can then transform the question to eigen question.

By RA formula, we can transfer it to (D-A)z = lambda * Dz

Here we can use the Laplacian matrix L = D_inverse * (D-A), and calculate the eigenvalues and eigenvectors from that. Then we can use the sign of it to determine the group the data in and the color of the dot in the graph. We will use the second-smallest eigenvalue to deal with the problem, since for the smallest eigenvalue the eigenvector is 1.


Here we compute the Laplacian matrix:

```python
L = D @ (D - A) #compute the L matrix
```

Here we find the eigenvalues and eigenvectors:

```python
Lam, U = np.linalg.eig(L) #apply the eigenvalue and vector function

```


```python
ix = Lam.argsort() #sort the function based on eigenvalue

Lam, U = Lam[ix], U[:,ix] #order it
```


```python
z_eig = U[:,1] #get the second eigenvector
```



Then we will graph the data based on the the eigenvector.

```python
col = np.where(z_eig >= -0.05, 'r', np.where(z_eig < -0.05, 'y' , 'b')) #assign the color to that
plt.scatter(X[:,0], X[:,1], c = col) #draw the graph based on the color
```


![png](/images/output_46_1.png)


We actually have a pretty nice clustering graph, using two color to distinguish the two curves.



## Part G

At last, we combine the procedures above and get the final result categorizing it to 1 and 0，which is the final clustering we get from our algorithm.

```python
def spectral_clustering(X, epsilon):
    '''
    inputs: X is the matrix that contains the euclidean coordinates;
            epsilon is the distance parameter
    
    purpose: in the function, we will use the distance and eigenvalue 
                to determine the binary group that the coordinates in,
                clustering the coordinates into two groups
    
    outputs: The function will output the label of each coordinate, 
                with 1 or 0

    assumptions: The assumptions of the function behind is that we can correctly clustering the point in this way;
                    also, the sign of the eigenvector after transformation and calculation tells about the group of the coordinate
    '''

    A = (pairwise_distances(X)  < epsilon ** 2).astype(int) #judging whether the distance is smaller than the epsilon, and change it to type int
    np.fill_diagonal(A, 0) #fill the diagonal as 0
    D = np.zeros((len(A), len(A))) #initialize d as zeros matrix
    np.fill_diagonal(D, sum(A[:,])) #fill the diagonal with the sum of column of the row of A
    Lam, U = np.linalg.eig(D @ (D - A)) #compute the eigenvalues and eigen vectors
    ix = Lam.argsort() #sort the eigenvalue
    Lam, U = Lam[ix], U[:,ix] #use the sorted version of the eigen function
    label = np.zeros(len(A)) #create a zero vector
    label[U[:,1] >0] = 1 #if eigenvector larger than 0 then label it 1
    
    return label
    
```

Here is the graph we gained:

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4)) #apply my clustering function to it as color
```



![png](/images/output_51_1.png)


## Part H

We then produced other data from make_moons and use it to test our algorithm.

```python
k = datasets.make_moons(n_samples = 1000, noise = 0)[0] #increase the noise to 3
plt.scatter(k[:,0], k[:,1], c = spectral_clustering(k, 0.4))
```


![png](/images/output_24_1.png)



```python
k = datasets.make_moons(n_samples = 1000, noise = 0.1)[0] #increase the noise to 3
plt.scatter(k[:,0], k[:,1], c = spectral_clustering(k, 0.4))
```


![png](/images/output_25_1.png)


The algorithm works well for noise of 0 and 0.1.



```python
k = datasets.make_moons(n_samples = 1000, noise = 0.2)[0] #increase the noise to 3
plt.scatter(k[:,0], k[:,1], c = spectral_clustering(k, 0.4))
```



![png](/images/output_26_1.png)



```python
k = datasets.make_moons(n_samples = 1000, noise = 0.3)[0] #increase the noise to 3
plt.scatter(k[:,0], k[:,1], c = spectral_clustering(k, 0.4))
```


![png](/images/output_27_1.png)



The algorithm does not work out as we increase the noise, since the data get scatter and scatter, makes it hard to group them based on distance.

## Part I

Now we will clustering bull's eye data set, which is a circle inside another circle.

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




![png](/images/output_59_1.png)


There are two concentric circles. K-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




![png](/images/output_61_1.png)



We will then apply our function to it, and see how things work out:

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.8))
```



![png](/images/output_63_1.png)



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.7))
```


![png](/images/output_64_1.png)



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.6))
```


![png](/images/output_65_1.png)



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```


![png](/images/output_66_1.png)



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```


![png](/images/output_67_1.png)



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.3))
```


![png](/images/output_68_1.png)


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.2))
```


![png](/images/output_69_1.png)


For value at 0.3， 0.4，and 0.5, I am able to correctly separate the two rings by using the function, and the graph look pretty nice.





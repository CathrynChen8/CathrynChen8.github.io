---
layout: post
title: Blog Post 5
---

We will try some models on cat and dog classification in this post.



## Load Packages and Obtain Data



We first import the library we need:

```python
import os
from tensorflow.keras import utils 
```


```python
import tensorflow as tf
```

We then obtain our data:

```python

# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

    Found 2000 files belonging to 2 classes.
    Found 1000 files belonging to 2 classes.


Here we use some magic to make it quicker to reading our data:

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

### Working with dataset

To see what we have in our dataset, we could access it using take, and here we will get some pictures of cat and some of dogs:

```python
import matplotlib.pyplot as plt
class_names = ['cat', 'dog']
plt.figure(figsize=(10, 10))
n = 0
m = 0
for images, labels in train_dataset.take(1) :
    for i in range(10):
      if labels[i] == 0: #if it is cat
        if n <= 2: #if get three pictures
          ax = plt.subplot(1, 3, n + 1)
          plt.imshow(images[i].numpy().astype("uint8"))
          n = n+1
          plt.title(class_names[labels[i]]) #title it as the category
          plt.axis("off")
plt.figure(figsize=(10, 10))
for images, labels in train_dataset.take(1) :
    for i in range(10):
      if labels[i] == 1: #if it is dog
        if m <= 2: #if get three picture
          ax = plt.subplot(1, 3, m + 1)
          plt.imshow(images[i].numpy().astype("uint8"))
          m = m+1
          plt.title(class_names[labels[i]]) #title it as the category
          plt.axis("off")


  


```


![png](/images/output_4_0.png)



![png](/images/output_4_1.png)


### Check Label Frequencies

```python
labels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()
```

```python
cat = 0
dog = 0
for labels in labels_iterator:
  if labels == 0:
    cat = cat + 1
  if labels == 1:
    dog = dog + 1

print(cat)
print(dog)
```

    1000
    1000


We have 1000 cats images and 1000 dog images in the training dataset, so we have baseline accuracy for about 0.5.


## First Model

We created a sequential model for this question:

```python
from tensorflow.keras import datasets, layers, models, losses


model1 =models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)), 
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.2),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(2) # number of classes
])
```

We can understand more about the model throught the summary of our model:

```python
model1.summary() 
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     conv2d (Conv2D)             (None, 158, 158, 32)      896       
                                                                     
     max_pooling2d (MaxPooling2D  (None, 79, 79, 32)       0         
     )                                                               
                                                                     
     dropout (Dropout)           (None, 79, 79, 32)        0         
                                                                     
     conv2d_1 (Conv2D)           (None, 77, 77, 64)        18496     
                                                                     
     max_pooling2d_1 (MaxPooling  (None, 38, 38, 64)       0         
     2D)                                                             
                                                                     
     flatten (Flatten)           (None, 92416)             0         
                                                                     
     dense (Dense)               (None, 64)                5914688   
                                                                     
     dropout_1 (Dropout)         (None, 64)                0         
                                                                     
     dense_1 (Dense)             (None, 2)                 130       
                                                                     
    =================================================================
    Total params: 5,934,210
    Trainable params: 5,934,210
    Non-trainable params: 0
    _________________________________________________________________

From the summary, we can see that we create a model with two conv2d layers, two drop out layers,  two maxpooling2d layer, two dense layers, and one flatten layer. The parameters is also shown in the table.


We report and compile our model:

```python
model1.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
history1 = model1.fit(train_dataset, 
                     epochs=20, 
                     validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 6s 86ms/step - loss: 106.6801 - accuracy: 0.5270 - val_loss: 0.6952 - val_accuracy: 0.4876
    Epoch 2/20
    63/63 [==============================] - 5s 83ms/step - loss: 0.6871 - accuracy: 0.5430 - val_loss: 0.6990 - val_accuracy: 0.4691
    Epoch 3/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6521 - accuracy: 0.6140 - val_loss: 0.7028 - val_accuracy: 0.5087
    Epoch 4/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6160 - accuracy: 0.6320 - val_loss: 0.7387 - val_accuracy: 0.5272
    Epoch 5/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.5633 - accuracy: 0.6820 - val_loss: 0.7691 - val_accuracy: 0.5272
    Epoch 6/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5128 - accuracy: 0.7080 - val_loss: 0.8680 - val_accuracy: 0.5322
    Epoch 7/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5054 - accuracy: 0.7400 - val_loss: 0.9253 - val_accuracy: 0.5359
    Epoch 8/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.4472 - accuracy: 0.7425 - val_loss: 1.0968 - val_accuracy: 0.5458
    Epoch 9/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.4354 - accuracy: 0.7890 - val_loss: 1.1773 - val_accuracy: 0.4938
    Epoch 10/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.3958 - accuracy: 0.7930 - val_loss: 1.4296 - val_accuracy: 0.5483
    Epoch 11/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.3599 - accuracy: 0.8090 - val_loss: 1.8180 - val_accuracy: 0.5507
    Epoch 12/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.3790 - accuracy: 0.8345 - val_loss: 1.8343 - val_accuracy: 0.5272
    Epoch 13/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.2858 - accuracy: 0.8710 - val_loss: 2.1097 - val_accuracy: 0.5446
    Epoch 14/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.2664 - accuracy: 0.8725 - val_loss: 1.8183 - val_accuracy: 0.5433
    Epoch 15/20
    63/63 [==============================] - 6s 83ms/step - loss: 0.2689 - accuracy: 0.8795 - val_loss: 2.2246 - val_accuracy: 0.5631
    Epoch 16/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.2015 - accuracy: 0.9120 - val_loss: 2.5413 - val_accuracy: 0.5693
    Epoch 17/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.1774 - accuracy: 0.9205 - val_loss: 3.0050 - val_accuracy: 0.5804
    Epoch 18/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.1554 - accuracy: 0.9305 - val_loss: 2.6142 - val_accuracy: 0.5495
    Epoch 19/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.1718 - accuracy: 0.9345 - val_loss: 2.7094 - val_accuracy: 0.5780
    Epoch 20/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.1570 - accuracy: 0.9375 - val_loss: 2.7281 - val_accuracy: 0.5693


Then we graph the accuracy and loss of training and validation:

```python
acc = history1.history['accuracy']
val_acc = history1.history['val_accuracy']

loss = history1.history['loss']
val_loss = history1.history['val_loss']
#graph the accuracy of training and validation dataset
plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')
#graph the loss of training and validation dataset
plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/output_11_0.png)




**The accuracy of my model stablized between 54% and 58% during training.**

Compare to the baseline, the accuracy is higher for 7%, better than the base line.

I observe somehow severe overfitting problem in my model, when the training accuracy is about 0.9375, the validation accuracy is only for about 0.5693.


## Model with Data Augmentation


We first see the application of randomflip and randomrotation function. We will do so by output the flip and rotation graph. We will apply the layer to one random image of our dataset and apply transformation to it.

This is the image transformation of the flip:

```python
data_flip = tf.keras.Sequential([
  tf.keras.layers.RandomFlip(),
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_flip(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


![png](/images/output_13_0.png)


This is the image transformation of the rotation:

```python
data_rotation = tf.keras.Sequential([
  tf.keras.layers.RandomRotation(0.2),
])

for image, _ in train_dataset.take(1):
  plt.figure(figsize=(10, 10))
  first_image = image[0]
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    augmented_image = data_rotation(tf.expand_dims(first_image, 0))
    plt.imshow(augmented_image[0] / 255)
    plt.axis('off')
```


![png](/images/output_14_0.png)


### Create Our Model with Data Augmentation Layer


We can do so by adding the augmentation layer on the model1:

```python
model2 = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'), #add the flip layer 
  tf.keras.layers.RandomRotation(0.2), #add the rotation layer
  layers.Conv2D(32, (3, 3), activation='relu'),
  layers.MaxPooling2D((2,2)),
  layers.Dropout(0.2),
  layers.Conv2D(64, (3, 3), activation='relu'),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(64, activation='relu'),
  layers.Dropout(0.2),
  layers.Dense(2) # number of classes
])
```

Here is our training history:

```python
model2.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])
history2 = model2.fit(train_dataset, 
                     epochs=20, 
                     validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 87ms/step - loss: 30.8213 - accuracy: 0.5330 - val_loss: 0.7017 - val_accuracy: 0.5099
    Epoch 2/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6855 - accuracy: 0.5650 - val_loss: 0.6918 - val_accuracy: 0.5347
    Epoch 3/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6767 - accuracy: 0.5805 - val_loss: 0.6930 - val_accuracy: 0.5161
    Epoch 4/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6657 - accuracy: 0.5900 - val_loss: 0.6966 - val_accuracy: 0.5012
    Epoch 5/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6819 - accuracy: 0.5685 - val_loss: 0.6919 - val_accuracy: 0.5198
    Epoch 6/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6641 - accuracy: 0.5940 - val_loss: 0.7055 - val_accuracy: 0.5681
    Epoch 7/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6704 - accuracy: 0.5855 - val_loss: 0.6694 - val_accuracy: 0.6027
    Epoch 8/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6633 - accuracy: 0.6050 - val_loss: 0.6742 - val_accuracy: 0.5804
    Epoch 9/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.6541 - accuracy: 0.6110 - val_loss: 0.6758 - val_accuracy: 0.5854
    Epoch 10/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6552 - accuracy: 0.6175 - val_loss: 0.6765 - val_accuracy: 0.5644
    Epoch 11/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6358 - accuracy: 0.6420 - val_loss: 0.6546 - val_accuracy: 0.6040
    Epoch 12/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6459 - accuracy: 0.6385 - val_loss: 0.6615 - val_accuracy: 0.5965
    Epoch 13/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6250 - accuracy: 0.6500 - val_loss: 0.6712 - val_accuracy: 0.5941
    Epoch 14/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6140 - accuracy: 0.6640 - val_loss: 0.6417 - val_accuracy: 0.6262
    Epoch 15/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6115 - accuracy: 0.6575 - val_loss: 0.6617 - val_accuracy: 0.5941
    Epoch 16/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6170 - accuracy: 0.6470 - val_loss: 0.6629 - val_accuracy: 0.5903
    Epoch 17/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6226 - accuracy: 0.6495 - val_loss: 0.6257 - val_accuracy: 0.6485
    Epoch 18/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6090 - accuracy: 0.6610 - val_loss: 0.6079 - val_accuracy: 0.6931
    Epoch 19/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6004 - accuracy: 0.6775 - val_loss: 0.6044 - val_accuracy: 0.6931
    Epoch 20/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.5975 - accuracy: 0.6795 - val_loss: 0.6393 - val_accuracy: 0.6399

Here is the graph of our model:

```python
acc = history2.history['accuracy']
val_acc = history2.history['val_accuracy']

loss = history2.history['loss']
val_loss = history2.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/output_15_0.png)


**The accuracy of my model stablized between 59% and 69% during training.**

Compare to model1, the validation accuracy is better for about 7%, which is a large improvement in some degree.

I observe that there is no much overfitting problem in my model, that the training accuracy is almost always smaller than  the validation accuracy.



## Data Preprocessing

We will apply data preprocessing in this model by adding the preprocessing layer before the model2:

```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i) #apply the data preprocessing
x = model2(x) #add model2 layer at last

model3 = tf.keras.Model(inputs = [i], outputs = [x]) #with imput i and output x

```

Here we get the training history:

```python
model3.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])


history3 = model3.fit(train_dataset, 
                     epochs=20, 
                     validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 7s 86ms/step - loss: 0.6848 - accuracy: 0.5525 - val_loss: 0.6550 - val_accuracy: 0.6015
    Epoch 2/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6646 - accuracy: 0.5820 - val_loss: 0.6418 - val_accuracy: 0.6213
    Epoch 3/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.6518 - accuracy: 0.6085 - val_loss: 0.6218 - val_accuracy: 0.6522
    Epoch 4/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6383 - accuracy: 0.6370 - val_loss: 0.6089 - val_accuracy: 0.6646
    Epoch 5/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6314 - accuracy: 0.6480 - val_loss: 0.6061 - val_accuracy: 0.6708
    Epoch 6/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6305 - accuracy: 0.6465 - val_loss: 0.5963 - val_accuracy: 0.6844
    Epoch 7/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6103 - accuracy: 0.6665 - val_loss: 0.5941 - val_accuracy: 0.6955
    Epoch 8/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5979 - accuracy: 0.6900 - val_loss: 0.5919 - val_accuracy: 0.6894
    Epoch 9/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.6122 - accuracy: 0.6675 - val_loss: 0.5792 - val_accuracy: 0.6770
    Epoch 10/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.6066 - accuracy: 0.6890 - val_loss: 0.5946 - val_accuracy: 0.6795
    Epoch 11/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5929 - accuracy: 0.6950 - val_loss: 0.5763 - val_accuracy: 0.7104
    Epoch 12/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5909 - accuracy: 0.6885 - val_loss: 0.5699 - val_accuracy: 0.7030
    Epoch 13/20
    63/63 [==============================] - 6s 84ms/step - loss: 0.5878 - accuracy: 0.6960 - val_loss: 0.5678 - val_accuracy: 0.6993
    Epoch 14/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5757 - accuracy: 0.7080 - val_loss: 0.5599 - val_accuracy: 0.7191
    Epoch 15/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5709 - accuracy: 0.7080 - val_loss: 0.5676 - val_accuracy: 0.7178
    Epoch 16/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5729 - accuracy: 0.7140 - val_loss: 0.5429 - val_accuracy: 0.7203
    Epoch 17/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.5704 - accuracy: 0.7115 - val_loss: 0.5417 - val_accuracy: 0.7290
    Epoch 18/20
    63/63 [==============================] - 6s 87ms/step - loss: 0.5626 - accuracy: 0.7210 - val_loss: 0.5506 - val_accuracy: 0.7203
    Epoch 19/20
    63/63 [==============================] - 6s 86ms/step - loss: 0.5504 - accuracy: 0.7245 - val_loss: 0.5614 - val_accuracy: 0.7005
    Epoch 20/20
    63/63 [==============================] - 6s 85ms/step - loss: 0.5590 - accuracy: 0.7190 - val_loss: 0.5470 - val_accuracy: 0.7141



Here is the graph of the accuracy and the loss:
```python
acc = history3.history['accuracy']
val_acc = history3.history['val_accuracy']

loss = history3.history['loss']
val_loss = history3.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/output_19_0.png)



**The accuracy of my model stablized between 67% and 73% during training.**

Compare to model1, the validation accuracy is better for about 14%, which is a large improvement.

I observe that there is no much overfitting problem in my model, that the training accuracy is almost always smaller than  the validation accuracy.


## Transfer Learning


Use these codes to download MobileNetV2.

```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

    Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5
    9412608/9406464 [==============================] - 0s 0us/step
    9420800/9406464 [==============================] - 0s 0us/step


We then build up the model with following structure:


1. The preprocessor layer from Part ยง4.
2. The data augmentation layers from Part ยง3.
3. The base_model_layer constructed above.
4. A Dense(2) layer at the very end to actually perform the classification.


To make it clearer, we seperate the data augmentation model here.

```python
data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
  tf.keras.layers.RandomRotation(0.2),
])
```

Then we build up the model as the structure said:

```python
i = tf.keras.Input(shape=(160, 160, 3)) #input 
x = tf.keras.applications.mobilenet_v2.preprocess_input(i) #prepocessing layer
x = data_augmentation(x) #data augmentation layer
x = base_model_layer(x) #base_model_layer given
x = tf.keras.layers.GlobalMaxPooling2D()(x) #globalmaxpooling layer
x = tf.keras.layers.Dropout(0.2)(x) #drop out layer to reduce overfitting problem
x = tf.keras.layers.Dense(2)(x) #dense the final output to 2 categories

model4 = tf.keras.Model(inputs = [i], outputs = [x])
```

We can understand more about the model by looking at the summary:

```python
model4.summary()
```

    Model: "model_4"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     input_10 (InputLayer)       [(None, 160, 160, 3)]     0         
                                                                     
     tf.math.truediv_7 (TFOpLamb  (None, 160, 160, 3)      0         
     da)                                                             
                                                                     
     tf.math.subtract_7 (TFOpLam  (None, 160, 160, 3)      0         
     bda)                                                            
                                                                     
     sequential_16 (Sequential)  (None, 160, 160, 3)       0         
                                                                     
     model_3 (Functional)        (None, 5, 5, 1280)        2257984   
                                                                     
     global_max_pooling2d_3 (Glo  (None, 1280)             0         
     balMaxPooling2D)                                                
                                                                     
     dropout_52 (Dropout)        (None, 1280)              0         
                                                                     
     dense_34 (Dense)            (None, 2)                 2562      
                                                                     
    =================================================================
    Total params: 2,260,546
    Trainable params: 2,562
    Non-trainable params: 2,257,984
    _________________________________________________________________

At last, we compile the model and print the training history:

```python
model4.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam', 
              metrics=['accuracy'])


history4 = model4.fit(train_dataset, 
                     epochs=20, 
                     validation_data= validation_dataset)
```

    Epoch 1/20
    63/63 [==============================] - 10s 106ms/step - loss: 0.3010 - accuracy: 0.9275 - val_loss: 0.1123 - val_accuracy: 0.9678
    Epoch 2/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2697 - accuracy: 0.9270 - val_loss: 0.0681 - val_accuracy: 0.9839
    Epoch 3/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2749 - accuracy: 0.9290 - val_loss: 0.0782 - val_accuracy: 0.9790
    Epoch 4/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.2500 - accuracy: 0.9350 - val_loss: 0.0649 - val_accuracy: 0.9827
    Epoch 5/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.2155 - accuracy: 0.9365 - val_loss: 0.0709 - val_accuracy: 0.9802
    Epoch 6/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2573 - accuracy: 0.9385 - val_loss: 0.0651 - val_accuracy: 0.9802
    Epoch 7/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2082 - accuracy: 0.9520 - val_loss: 0.0703 - val_accuracy: 0.9802
    Epoch 8/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.2093 - accuracy: 0.9495 - val_loss: 0.0803 - val_accuracy: 0.9777
    Epoch 9/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2073 - accuracy: 0.9430 - val_loss: 0.0643 - val_accuracy: 0.9851
    Epoch 10/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1631 - accuracy: 0.9545 - val_loss: 0.0618 - val_accuracy: 0.9802
    Epoch 11/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.1628 - accuracy: 0.9510 - val_loss: 0.0728 - val_accuracy: 0.9814
    Epoch 12/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1675 - accuracy: 0.9575 - val_loss: 0.0995 - val_accuracy: 0.9728
    Epoch 13/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.2072 - accuracy: 0.9430 - val_loss: 0.0452 - val_accuracy: 0.9864
    Epoch 14/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1930 - accuracy: 0.9520 - val_loss: 0.0622 - val_accuracy: 0.9839
    Epoch 15/20
    63/63 [==============================] - 6s 90ms/step - loss: 0.2002 - accuracy: 0.9485 - val_loss: 0.0561 - val_accuracy: 0.9851
    Epoch 16/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1519 - accuracy: 0.9520 - val_loss: 0.0753 - val_accuracy: 0.9827
    Epoch 17/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1802 - accuracy: 0.9540 - val_loss: 0.0579 - val_accuracy: 0.9802
    Epoch 18/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1469 - accuracy: 0.9560 - val_loss: 0.0617 - val_accuracy: 0.9827
    Epoch 19/20
    63/63 [==============================] - 6s 92ms/step - loss: 0.1654 - accuracy: 0.9550 - val_loss: 0.0493 - val_accuracy: 0.9864
    Epoch 20/20
    63/63 [==============================] - 6s 91ms/step - loss: 0.1628 - accuracy: 0.9550 - val_loss: 0.0524 - val_accuracy: 0.9839

We can also graph the accuracy and loss of our training history:

```python
acc = history4.history['accuracy']
val_acc = history4.history['val_accuracy']

loss = history4.history['loss']
val_loss = history4.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()
```


![png](/images/output_25_0.png)



**The accuracy of my model stablized between 97% and 99% during training.**

Compare to all other models, for example model3, the validation accuracy is better for about 27%, which is a large improvement.

I observe that there is no overfitting problem in my model, that the training accuracy is almost always smaller than  the validation accuracy.


## Score on Test Data

We will at last apply our best model, model4 to the test dataset
```python
loss, accuracy = model4.evaluate(test_dataset) #evaluate the function on test_dataset
print('Test accuracy :', accuracy)
```

    6/6 [==============================] - 1s 65ms/step - loss: 0.0609 - accuracy: 0.9896
    Test accuracy : 0.9895833134651184

The performance of my model is pretty nice, for about 99% accuracy.




